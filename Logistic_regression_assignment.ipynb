{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Question & Answer**"
      ],
      "metadata": {
        "id": "2kUpbGoeJ0ae"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1. What is Logistic Regression, and how does it differ from Linear Regression ?**\n",
        "- Logistic Regression and Linear Regression are both supervised learning algorithms used in machine learning, but they are used for different types of problems and have key differences in terms of output, purpose, and function.\n",
        "\n",
        "1. Purpose\n",
        "\n",
        "| Aspect       | Logistic Regression                         | Linear Regression                                  |\n",
        "| ------------ | ------------------------------------------- | -------------------------------------------------- |\n",
        "| **Used For** | Classification problems (e.g., yes/no, 0/1) | Regression problems (predicting continuous values) |\n",
        "\n",
        "2. Output\n",
        "\n",
        "| Aspect               | Logistic Regression                           | Linear Regression                            |\n",
        "| -------------------- | --------------------------------------------- | -------------------------------------------- |\n",
        "| **Output Type**      | Probability (between 0 and 1)                 | Continuous numerical value (any real number) |\n",
        "| **Final Prediction** | Usually converted to 0 or 1 using a threshold | Used directly as a continuous value          |\n",
        "\n",
        "3. Mathematical Function\n",
        "\n",
        "Linear Regression Equation:\n",
        "                  \n",
        "                  y=β0​+β1​x1​+β2​x2​+…+βn​xn​\n",
        "\t​\n",
        "\n",
        "This outputs any real number.\n",
        "\n",
        "Logistic Regression Equation:\n",
        "\n",
        "                    p= 1/ 1+e−(β0​+β1​x1​+β2​x2​+…+βn​xn​)\n",
        "\n",
        "This uses the sigmoid function to squeeze output between 0 and 1.\n",
        "\n",
        "4. Interpretation : Linear Regression tries to model the relationship between inputs and a real-valued output.\n",
        "Logistic Regression models the probability that an instance belongs to a class.\n",
        "\n",
        "5. Loss Function : Linear Regression: Uses Mean Squared Error (MSE).\n",
        "Logistic Regression: Uses Log Loss (also called Cross-Entropy Loss).\n",
        "\n",
        "6. Example\n",
        "\n",
        "| Problem                             | Suitable Algorithm  |\n",
        "| ----------------------------------- | ------------------- |\n",
        "| Predicting house prices             | Linear Regression   |\n",
        "| Predicting whether an email is spam | Logistic Regression |\n",
        "| Estimating a person's weight        | Linear Regression   |\n",
        "| Classifying if a tumor is malignant | Logistic Regression |\n",
        "\n"
      ],
      "metadata": {
        "id": "kozXosXbJ-Ay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2. Explain the role of the Sigmoid function in Logistic Regression?**\n",
        "-  The Sigmoid function plays a central role in Logistic Regression — it's what allows the model to convert a linear combination of inputs into a probability value between 0 and 1, which is essential for classification tasks.\n",
        "The Sigmoid function (also called the logistic function) is defined as:\n",
        "\n",
        "                                   σ(z)= 1/ 1+e−z\n",
        "\n",
        "Role of the Sigmoid Function in Logistic Regression\n",
        "1. Transforms Linear Output into Probability:\n",
        "- In logistic regression, we compute a linear combination of input features:                                \n",
        "                         z=β0​+β1​x1​+…+βn​xn​\n",
        "- This value z can be any real number.\n",
        "- The sigmoid function maps z into the range (0, 1), which can be - interpreted as a probability.                         \n",
        "\n",
        "2. Helps Make Binary Decisions:\n",
        "- After converting the output to a probability, we apply a threshold (commonly 0.5):\n",
        "\n",
        "- If  σ(z)≥0.5: predict class 1   \n",
        "- If  σ(z)<0.5: predict class 0\n",
        "\n",
        "3. Supports Gradient-Based Optimization:\n",
        "\n",
        "- The sigmoid function is differentiable, which means it works well with optimization algorithms like Gradient Descent, used to minimize the log loss in logistic regression.\n",
        "\n",
        "4. Graph of the Sigmoid Function\n",
        "\n",
        "- S-shaped (sigmoid curve)\n",
        "\n",
        "- Output is close to 0 when z≪0\n",
        "\n",
        "- Output is close to 1 when z≫0\n",
        "\n",
        "- Output is 0.5 when z=0"
      ],
      "metadata": {
        "id": "Ru1QR8hJNUmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "     |\n",
        "  1  |                              *\n",
        "     |                         *\n",
        "  0.5|---------------*-------------------> z\n",
        "     |        *\n",
        "  0  | *\n",
        "     |\n"
      ],
      "metadata": {
        "id": "KemoG6E7PzSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3. What is Regularization in Logistic Regression and why is it needed?**\n",
        "-  Regularization is a technique used to prevent overfitting in machine learning models by penalizing large coefficients (weights) in the model.\n",
        "\n",
        " In the context of Logistic Regression, regularization adds a penalty term to the loss function to discourage the model from fitting the training data too closely (which could hurt performance on new, unseen data).\n",
        "\n",
        "Regularization is needed in logistic regression because:\n",
        "- Without regularization, a logistic regression model can:\n",
        "- Learn very large weights for some features.\n",
        "- Overfit the training data (especially if there are many features or noisy data).\n",
        "- Overfitting means the model performs well on training data but poorly on unseen/test data.\n",
        "- Regularization helps the model generalize better."
      ],
      "metadata": {
        "id": "8s1SgHAIP2d-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4. What are some common evaluation metrics for classification models, and why are they important?**\n",
        "-  Metrics help you understand how well your model performs, beyond just accuracy.\n",
        "- They guide you in choosing the best model for your application.\n",
        "- Different problems (e.g., fraud detection, medical diagnosis) require different priorities (e.g., minimizing false negatives vs. false positives).\n",
        "They are important because:\n",
        "1. Accuracy :- Proportion of correct predictions out of all predictions.\n",
        "\n",
        "                Accuracy=TP+TN+FP+FNTP+TN​\n",
        "- Use Case: When classes are balanced.\n",
        "- Limitation: Misleading when classes are imbalanced.\n",
        "\n",
        "2. Precision:- Out of all predicted positives, how many were actually positive\n",
        "                 Precision= TP/ TP+FP\n",
        "\n",
        "- Use Case: Important when false positives are costly (e.g., spam filters)\n",
        "3. Recall (Sensitivity or True Positive Rate):- Out of all actual positives, how many were correctly predicted?\n",
        "\n",
        "                     Recall=TP/ TP+FN\n",
        "- Use Case: Important when false negatives are costly (e.g., cancer detection).\n",
        "\n",
        "4. F1 Score:-Harmonic mean of precision and recall.\n",
        "\n",
        "           F1 Score=2 x Precision x Recall/ Precision+Recall\n",
        "\n",
        "Use Case: When you want a balance between precision and recall.\n",
        "\n",
        "Useful for imbalanced classes.\n",
        "\n",
        "5. Confusion Matrix\n",
        "A table showing: True Positives (TP), True Negatives (TN), False Positives (FP), False Negatives (FN), Helps understand types of errors the model makes.\n",
        "\n",
        "6. ROC Curve & AUC (Area Under the Curve)\n",
        "- ROC Curve: Plots True Positive Rate vs. False Positive Rate at different thresholds.\n",
        "- AUC (Area Under Curve): Measures the model’s ability to distinguish between classes.\n",
        "- AUC = 1: Perfect classifier\n",
        "- AUC = 0.5: Random guessing\n",
        "- Use Case: Great for comparing models across different threshold settings.\n"
      ],
      "metadata": {
        "id": "LFF-Q1_hraK2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5. Write a Python program that loads a CSV file into a Pandas DataFrame,\n",
        "splits into train/test sets, trains a Logistic Regression model, and prints its accuracy.(Use Dataset from sklearn package)**"
      ],
      "metadata": {
        "id": "2ste86i-uCJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f1m9gmcvccR",
        "outputId": "177c4165-dc53-4048-fd62-ebef93db03a5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q6. Write a Python program to train a Logistic Regression model using L2\n",
        "regularization (Ridge) and print the model coeffecients and accuracy.**"
      ],
      "metadata": {
        "id": "oEFko_aWxziz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(penalty='l2', C=1.0, solver='liblinear', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Model Coefficients:\")\n",
        "for feature, coef in zip(X.columns, model.coef_[0]):\n",
        "    print(f\"{feature}: {coef:.4f}\")\n",
        "\n",
        "print(f\"\\nTest Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yil01pS24A2i",
        "outputId": "033414ef-2429-4d9b-9865-68f1db5a63fe"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Coefficients:\n",
            "mean radius: 2.1325\n",
            "mean texture: 0.1528\n",
            "mean perimeter: -0.1451\n",
            "mean area: -0.0008\n",
            "mean smoothness: -0.1426\n",
            "mean compactness: -0.4156\n",
            "mean concavity: -0.6519\n",
            "mean concave points: -0.3445\n",
            "mean symmetry: -0.2076\n",
            "mean fractal dimension: -0.0298\n",
            "radius error: -0.0500\n",
            "texture error: 1.4430\n",
            "perimeter error: -0.3039\n",
            "area error: -0.0726\n",
            "smoothness error: -0.0162\n",
            "compactness error: -0.0019\n",
            "concavity error: -0.0449\n",
            "concave points error: -0.0377\n",
            "symmetry error: -0.0418\n",
            "fractal dimension error: 0.0056\n",
            "worst radius: 1.2321\n",
            "worst texture: -0.4046\n",
            "worst perimeter: -0.0362\n",
            "worst area: -0.0271\n",
            "worst smoothness: -0.2626\n",
            "worst compactness: -1.2090\n",
            "worst concavity: -1.6180\n",
            "worst concave points: -0.6153\n",
            "worst symmetry: -0.7428\n",
            "worst fractal dimension: -0.1170\n",
            "\n",
            "Test Accuracy: 0.9561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q7. Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr' and print the classification report.(Use Dataset from sklearn package)**"
      ],
      "metadata": {
        "id": "bOJjwnRN4EUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "data = load_iris()\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IBzHy496jQ5",
        "outputId": "8a88a36b-4fc2-4669-f5c3-c29b2ecd7449"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        10\n",
            "  versicolor       1.00      1.00      1.00         9\n",
            "   virginica       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q8. Write a Python program to apply GridSearchCV to tune C and penalty\n",
        "hyperparameters for Logistic Regression and print the best parameters and validation accuracy**"
      ],
      "metadata": {
        "id": "Bmt1oNTo6tTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model_no_scaling = LogisticRegression(max_iter=10000)\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_with_scaling = LogisticRegression(max_iter=10000)\n",
        "model_with_scaling.fit(X_train_scaled, y_train)\n",
        "y_pred_with_scaling = model_with_scaling.predict(X_test_scaled)\n",
        "accuracy_with_scaling = accuracy_score(y_test, y_pred_with_scaling)\n",
        "\n",
        "print(f\"Accuracy WITHOUT scaling: {accuracy_no_scaling:.4f}\")\n",
        "print(f\"Accuracy WITH scaling   : {accuracy_with_scaling:.4f}\")\n"
      ],
      "metadata": {
        "id": "9pN60KLn-IYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q9. Write a Python program to standardize the features before training Logistic Regression and compare the model's accuracy with and without scaling**"
      ],
      "metadata": {
        "id": "WekkdGyT8syw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model_no_scaling = LogisticRegression(max_iter=10000)\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_with_scaling = LogisticRegression(max_iter=10000)\n",
        "model_with_scaling.fit(X_train_scaled, y_train)\n",
        "y_pred_with_scaling = model_with_scaling.predict(X_test_scaled)\n",
        "accuracy_with_scaling = accuracy_score(y_test, y_pred_with_scaling)\n",
        "\n",
        "print(f\"Accuracy WITHOUT scaling: {accuracy_no_scaling:.4f}\")\n",
        "print(f\"Accuracy WITH scaling   : {accuracy_with_scaling:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVoY3-T4870l",
        "outputId": "637e9eff-e961-4043-aacc-bc57d3e4ebd3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy WITHOUT scaling: 0.9561\n",
            "Accuracy WITH scaling   : 0.9737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q10. Imagine you are working at an e-commerce company that wants to\n",
        "predict which customers will respond to a marketing campaign. Given an imbalanced dataset (only 5% of customers respond), describe the approach you’d take to build a Logistic Regression model — including data handling, feature scaling, balancing classes, hyperparameter tuning, and evaluating the model for this real-world business use case.**"
      ],
      "metadata": {
        "id": "vHWKfiQqtnJe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression(class_weight='balanced')\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear'],\n",
        "    'class_weight': ['balanced']\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, scoring='f1', cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid.best_estimator_\n",
        "\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "\n",
        "y_pred = best_model.predict(X_test)\n",
        "y_proba = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"ROC-AUC:\", roc_auc_score(y_test, y_proba))"
      ],
      "metadata": {
        "id": "csid_KqP8sXw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}